{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e489ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/toytrans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from toy_model import *\n",
    "from metrics import *\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8a601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mce24b119\u001b[0m (\u001b[33mjerrycloud3316-ai-club-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e397412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0.5]\n",
    "])\n",
    "\n",
    "T1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c82149",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0_proc1 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1], \n",
    "    [0, 0, 0.5]\n",
    "])\n",
    "T1_proc1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "# Different process\n",
    "'''T0_proc2 = np.array([\n",
    "    [0.3, 0.7, 0],\n",
    "    [0, 0.2, 0.8],\n",
    "    [0.1, 0.1, 0.8]\n",
    "])\n",
    "T1_proc2 = np.array([\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0.6, 0.4, 0],\n",
    "    [0, 0.8, 0.2]\n",
    "])'''\n",
    "process1 = MarkovData(n_gen=50, gen_len=32, n_states=3, d_vocab=2, T_list=[T0_proc1, T1_proc1], seed=42)\n",
    "#process2 = MarkovData(n_gen=50, gen_len=30, n_states=3, d_vocab=2, T_list=[T0_proc2, T1_proc2], seed=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_data="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3134a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config = MetricsConfig(\n",
    "    track_markov_kl=True,\n",
    "    markov_processes=[process1],  # Will create markov_kl_proc0, markov_kl_proc1\n",
    "    \n",
    "    \n",
    "    track_ngrams=True,\n",
    "    ngram_orders=[1, 2, 3],\n",
    "    track_previous_token=True,\n",
    "    track_in_context=True, \n",
    "    #icl_data=icl_data,\n",
    "    icl_k1=5,\n",
    "    icl_k2=32,\n",
    "\n",
    "    track_prefix_matching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2924bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarkovData(10000, 32, 3, 2, [T0, T1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb26fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sbhandari/Documents/GitHub/Vivekafork/toy_transformer/wandb/run-20250914_131533-vfr6f0np</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL/runs/vfr6f0np' target=\"_blank\">vibrant-hill-17</a></strong> to <a href='https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL' target=\"_blank\">https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL/runs/vfr6f0np' target=\"_blank\">https://wandb.ai/jerrycloud3316-ai-club-iit-madras/ICL/runs/vfr6f0np</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:01<07:36,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss 0.488627165555954\n",
      "Metrics logged at step 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:03<08:44,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss 0.47836923599243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/300 [00:04<08:06,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss 0.47260782122612\n",
      "Metrics logged at step 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:06<08:35,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss 0.4706459939479828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/300 [00:08<08:09,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 600\n",
      "Epoch 5 Validation Loss 0.46251600980758667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/300 [00:10<08:35,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Loss 0.4584489166736603\n",
      "Metrics logged at step 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/300 [00:11<08:16,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Loss 0.45914945006370544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/300 [00:13<08:40,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 1000\n",
      "Epoch 8 Validation Loss 0.45583707094192505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/300 [00:15<08:20,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Loss 0.4598698318004608\n",
      "Metrics logged at step 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/300 [00:17<08:42,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Samples 8000 Step 124 Training Loss 0.4502628445625305\n",
      "Epoch 10 Validation Loss 0.4552217721939087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/300 [00:18<08:21,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Loss 0.4607268273830414\n",
      "Metrics logged at step 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/300 [00:21<08:47,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Loss 0.45436108112335205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 13/300 [00:22<08:19,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 1600\n",
      "Epoch 13 Validation Loss 0.45651790499687195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 14/300 [00:24<08:37,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Loss 0.4631907045841217\n",
      "Metrics logged at step 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 15/300 [00:26<08:10,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Loss 0.45579928159713745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 16/300 [00:28<08:29,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 2000\n",
      "Epoch 16 Validation Loss 0.46020305156707764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 17/300 [00:29<08:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Loss 0.4529959559440613\n",
      "Metrics logged at step 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 18/300 [00:31<08:19,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Loss 0.45403653383255005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 19/300 [00:32<07:55,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Loss 0.45806702971458435\n",
      "Metrics logged at step 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/300 [00:34<08:14,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Samples 8000 Step 124 Training Loss 0.4549579322338104\n",
      "Epoch 20 Validation Loss 0.4550844132900238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 21/300 [00:36<07:55,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 2600\n",
      "Epoch 21 Validation Loss 0.45116204023361206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 22/300 [00:38<08:14,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Validation Loss 0.45509809255599976\n",
      "Metrics logged at step 2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 23/300 [00:39<07:55,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Validation Loss 0.44951385259628296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 24/300 [00:41<08:17,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 3000\n",
      "Epoch 24 Validation Loss 0.4526413381099701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 25/300 [00:43<08:07,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Validation Loss 0.44996392726898193\n",
      "Metrics logged at step 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 26/300 [00:45<08:41,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Validation Loss 0.44736799597740173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 27/300 [00:47<08:53,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Validation Loss 0.44652363657951355\n",
      "Metrics logged at step 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 28/300 [00:50<09:02,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Validation Loss 0.445494145154953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 29/300 [00:51<08:37,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 3600\n",
      "Epoch 29 Validation Loss 0.44917237758636475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 30/300 [00:53<08:45,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Samples 8000 Step 124 Training Loss 0.43920472264289856\n",
      "Epoch 30 Validation Loss 0.4432852864265442\n",
      "Metrics logged at step 3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 31/300 [00:55<08:21,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Validation Loss 0.44323283433914185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 32/300 [00:57<08:31,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 4000\n",
      "Epoch 32 Validation Loss 0.44239798188209534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 33/300 [00:59<08:06,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Validation Loss 0.43990573287010193\n",
      "Metrics logged at step 4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 34/300 [01:01<08:24,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Validation Loss 0.43843868374824524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 35/300 [01:02<07:49,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Validation Loss 0.44146981835365295\n",
      "Metrics logged at step 4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 36/300 [01:04<08:02,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Validation Loss 0.4356015622615814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 37/300 [01:06<07:32,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 4600\n",
      "Epoch 37 Validation Loss 0.436771035194397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 38/300 [01:08<07:49,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Validation Loss 0.43330469727516174\n",
      "Metrics logged at step 4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 39/300 [01:09<07:24,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Validation Loss 0.4321878254413605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [01:11<07:36,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 5000\n",
      "Epoch 40 Samples 8000 Step 124 Training Loss 0.4292237162590027\n",
      "Epoch 40 Validation Loss 0.432025671005249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 41/300 [01:13<07:21,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Validation Loss 0.4321744441986084\n",
      "Metrics logged at step 5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 42/300 [01:15<07:43,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Validation Loss 0.42898932099342346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 43/300 [01:16<07:19,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Validation Loss 0.4278061091899872\n",
      "Metrics logged at step 5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 44/300 [01:18<07:30,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Validation Loss 0.42607051134109497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 45/300 [01:19<07:10,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 5600\n",
      "Epoch 45 Validation Loss 0.4256940484046936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 46/300 [01:21<07:33,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Validation Loss 0.42135244607925415\n",
      "Metrics logged at step 5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 47/300 [01:23<07:25,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Validation Loss 0.43536490201950073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 48/300 [01:25<07:31,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 6000\n",
      "Epoch 48 Validation Loss 0.41775375604629517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 49/300 [01:27<07:24,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Validation Loss 0.415676087141037\n",
      "Metrics logged at step 6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50/300 [01:29<07:34,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Samples 8000 Step 124 Training Loss 0.4119405746459961\n",
      "Epoch 50 Validation Loss 0.4138227105140686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 51/300 [01:30<07:12,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Validation Loss 0.41310131549835205\n",
      "Metrics logged at step 6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 52/300 [01:32<07:22,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Validation Loss 0.414558082818985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 53/300 [01:34<07:13,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 6600\n",
      "Epoch 53 Validation Loss 0.41278597712516785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 54/300 [01:36<07:32,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Validation Loss 0.4105353355407715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 54/300 [01:37<07:22,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics logged at step 6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msilu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Training\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Logging\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_project_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mICL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproc1/debug/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ALL ADVANCED METRICS ENABLED\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_log_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/Documents/GitHub/Vivekafork/toy_transformer/toy_model.py:406\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset, n_layers, d_model, n_heads, d_head, attn_only, d_mlp, act_fn, normalization_type, positional_embedding_type, n_epochs, batch_size, lr, optimizer_name, wandb, wandb_project_name, device, seed, save_every, save_dir, print_every, eval_every, val_frac, metrics_config, metrics_log_interval, track_ngrams, ngram_orders, track_previous_token, track_in_context, track_composition, track_prefix_matching)\u001b[0m\n\u001b[1;32m    404\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m Subset(dataset, train_indices)\n\u001b[1;32m    405\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m Subset(dataset, val_indices)\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train(model, train_cfg, dataset, metrics_config\u001b[38;5;241m=\u001b[39mmetrics_config)\n",
      "File \u001b[0;32m~/Documents/GitHub/Vivekafork/toy_transformer/toy_model.py:179\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, train_data, val_data, eval_every, metrics_config, metrics_log_interval)\u001b[0m\n\u001b[1;32m    176\u001b[0m tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    178\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(tokens, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 179\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmax_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/toytrans/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/toytrans/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/toytrans/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = train_model(\n",
    "    dataset=dataset,\n",
    "    n_layers=2,\n",
    "    d_model=16,\n",
    "    n_heads=2, \n",
    "    attn_only=True,\n",
    "    act_fn='silu',\n",
    "\n",
    "    # Training\n",
    "    n_epochs=300,\n",
    "    batch_size=64,\n",
    "    lr=0.1,\n",
    "\n",
    "    # Logging\n",
    "    wandb=True,\n",
    "    wandb_project_name=\"ICL\",\n",
    "    save_dir=\"proc1/debug/\",\n",
    "    save_every=20,\n",
    "    print_every=10,\n",
    "\n",
    "    # ALL ADVANCED METRICS ENABLED\n",
    "    metrics_config=metrics_config,\n",
    "    metrics_log_interval=50)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7790ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Codes\n",
    "\n",
    "def bigram_kl(model):\n",
    "    T0 = np.array([\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 0.5]\n",
    "    ])\n",
    "\n",
    "    T1 = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0.5, 0, 0]\n",
    "    ])\n",
    "\n",
    "    # Generate test data\n",
    "    test_data = MarkovData(50, 30, 3, 2, [T0, T1], seed=42)\n",
    "    x = torch.stack(test_data.data)  # shape: (50, 30)\n",
    "\n",
    "    # Build dist1 manually based on x values\n",
    "    dist1 = torch.zeros(x.size(0), x.size(1), 2)  # shape: (50, 30, 2)\n",
    "    dist1[..., 0] = torch.where(x == 1, 1, 0.5)  # P(0)\n",
    "    dist1[..., 1] = torch.where(x == 1, 0, 0.5)  # P(1)\n",
    "\n",
    "    # Get predicted probabilities from model\n",
    "    dist2 = model(x).softmax(dim=-1)  # shape: (50, 30, 2)\n",
    "\n",
    "    # Avoid log(0) by clamping very small values\n",
    "    eps = 1e-8\n",
    "    dist1_clamped = dist1.clamp(min=eps)\n",
    "    dist2_clamped = dist2.clamp(min=eps)\n",
    "\n",
    "    # Compute KL divergence manually: sum_i P(i) * log(P(i)/Q(i))\n",
    "    def kl(dist1_clamped, dist2_clamped):\n",
    "        kl = dist1_clamped * (dist1_clamped.log() - dist2_clamped.log())  # shape: (4, 10, 2)\n",
    "        kl_sum = kl.sum(dim=-1)  # sum over distribution axis → shape: (4, 10)\n",
    "        return kl_sum.mean().item()  # scalar\n",
    "\n",
    "    return kl(dist1_clamped, dist2_clamped), kl(dist2_clamped, dist1_clamped)\n",
    "\n",
    "def trigram_kl(model):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between true Mealy process and model predictions\n",
    "    based on trigram-level distributions.\n",
    "    \"\"\"\n",
    "    T0 = np.array([\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 0.5]\n",
    "    ])\n",
    "\n",
    "    T1 = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0.5, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Generate test data\n",
    "    test_data = MarkovData(50, 32, 3, 2, [T0, T1], seed=42)\n",
    "    x = torch.stack(test_data.data)  # shape: (50, 32)\n",
    "    \n",
    "    batch_size, seq_len = x.shape\n",
    "    dist1 = torch.zeros(batch_size, seq_len, 2)  # shape: (50, 32, 2)\n",
    "    \n",
    "    # For trigram-based prediction, we need the last two tokens to predict the next\n",
    "    for i in range(batch_size):\n",
    "        for j in range(2, seq_len):  \n",
    "            prev_bigram = f\"{x[i, j-2].item()}{x[i, j-1].item()}\"\n",
    "            \n",
    "            if prev_bigram == \"00\":\n",
    "                dist1[i, j, 0] = 0.5    # P(0|00) \n",
    "                dist1[i, j, 1] = 0.5    # P(1|00)\n",
    "            elif prev_bigram == \"01\":\n",
    "                dist1[i, j, 0] = 1.0    # P(0|01)\n",
    "                dist1[i, j, 1] = 0.0    # P(1|01)\n",
    "            elif prev_bigram == \"10\":\n",
    "                dist1[i, j, 0] = 1.0    # P(0|10)\n",
    "                dist1[i, j, 1] = 0.0    # P(1|10)\n",
    "            elif prev_bigram == \"11\":\n",
    "                # This should never occur in the true process\n",
    "                dist1[i, j, 0] = 0.5    # Fallback\n",
    "                dist1[i, j, 1] = 0.5\n",
    "    \n",
    "    # Get predicted probabilities from model\n",
    "    dist2 = model(x).softmax(dim=-1)  # shape: (50, 32, 2)\n",
    "    \n",
    "    # Only evaluate positions where we have trigram context (positions 2 and beyond)\n",
    "    dist1_eval = dist1[:, 2:, :]\n",
    "    dist2_eval = dist2[:, 2:, :]\n",
    "    \n",
    "    # Avoid log(0) by clamping very small values\n",
    "    eps = 1e-8\n",
    "    dist1_clamped = dist1_eval.clamp(min=eps)\n",
    "    dist2_clamped = dist2_eval.clamp(min=eps)\n",
    "    \n",
    "    # Compute KL divergence manually: sum_i P(i) * log(P(i)/Q(i))\n",
    "    def kl(dist1_clamped, dist2_clamped):\n",
    "        kl_div = dist1_clamped * (dist1_clamped.log() - dist2_clamped.log())\n",
    "        kl_sum = kl_div.sum(dim=-1)  # sum over distribution axis\n",
    "        return kl_sum.mean().item()  # scalar\n",
    "    \n",
    "    return kl(dist1_clamped, dist2_clamped), kl(dist2_clamped, dist1_clamped)\n",
    "\n",
    "def markov_kl(model):\n",
    "    T0 = np.array([\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 0.5]\n",
    "    ])\n",
    "\n",
    "    T1 = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0.5, 0, 0]\n",
    "    ])\n",
    "\n",
    "    # Generate test data\n",
    "    test_data = MarkovData(50, 30, 3, 2, [T0, T1], seed=42)\n",
    "    x = torch.stack(test_data.data)  # shape: (4, 10)\n",
    "    dist1 = []\n",
    "    for etas in test_data.states:\n",
    "        dist1.append([test_data.model.token_probabilities(eta) for eta in etas])\n",
    "    dist1 = torch.tensor(np.array(dist1))[:, 1:, :]\n",
    "\n",
    "    # Get predicted probabilities from model\n",
    "    dist2 = model(x).softmax(dim=-1)  # shape: (4, 10, 2)\n",
    "\n",
    "    # Avoid log(0) by clamping very small values\n",
    "    eps = 1e-8\n",
    "    dist1_clamped = dist1.clamp(min=eps)\n",
    "    dist2_clamped = dist2.clamp(min=eps)\n",
    "\n",
    "    # Compute KL divergence manually: sum_i P(i) * log(P(i)/Q(i))\n",
    "    def kl(dist1_clamped, dist2_clamped):\n",
    "        kl = dist1_clamped * (dist1_clamped.log() - dist2_clamped.log())  # shape: (4, 10, 2)\n",
    "        kl_sum = kl.sum(dim=-1)  # sum over distribution axis → shape: (4, 10)\n",
    "        return kl_sum.mean().item()  # scalar\n",
    "\n",
    "    return kl(dist1_clamped, dist2_clamped), kl(dist2_clamped, dist1_clamped)\n",
    "\n",
    "def test_on_all(model, gen_len, start=2):\n",
    "    assert gen_len % 3 == 0, 'gen_len should be a multiple of 3'\n",
    "    # Building the test data\n",
    "    test_seq = [0, 1, None] * (gen_len // 3)\n",
    "    all_seq = [test_seq]\n",
    "    temp = []\n",
    "    for i in range(2, len(test_seq), 3):\n",
    "        for j in range(len(all_seq)):\n",
    "            temp.append(all_seq[j].copy())\n",
    "            temp[-1][i] = 0\n",
    "            temp.append(all_seq[j].copy())\n",
    "            temp[-1][i] = 1\n",
    "        all_seq, temp = temp, []\n",
    "    all_seq = [all_seq, [i[-1:] + i[:-1] for i in all_seq], [i[-2:] + i[:-2] for i in all_seq]]\n",
    "    all_seq = [torch.tensor(seq, dtype=torch.int64) for seq in all_seq]\n",
    "    \n",
    "    # Testing the model\n",
    "    errors = []\n",
    "    acc = 0.0\n",
    "    for i in range(3):\n",
    "        preds = model(all_seq[i])[:,:-1,:].argmax(dim=-1)\n",
    "        err = torch.where(all_seq[i][:,1:] != preds, 1, 0)\n",
    "        for j in range(gen_len // 3):\n",
    "            err[:, (3*j + i + 1) % (gen_len - 1)] = 0\n",
    "        err[:,:start] = 0\n",
    "        errors.append(err)\n",
    "        acc += (1 - err.sum() / (err.shape[0] * err.shape[1])) * 100\n",
    "    acc /= 3\n",
    "    print(f'Accuracy: {acc:.2f} %')\n",
    "    return torch.cat([all_seq[i][errors[i].sum(dim=-1).nonzero().flatten().tolist()] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af915678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.6185, -4.2362],\n",
      "        [ 9.8626, -8.1916],\n",
      "        [ 5.7849, -4.7819]], grad_fn=<SelectBackward0>)\n",
      "tensor([0, 0, 0])\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"proc1/seq_len_30/model300.pt\",\"proc1/seq_len_30/model_cfg.pt\")\n",
    "logits = model(torch.tensor([[0,1,1,0,1,0,0,1,1,0], \n",
    "                                [1,0,1,1,0,1,0,0,1,1], \n",
    "                                [1,0,0,1,0,0,1,0,0,1]], dtype=torch.int64))\n",
    "print(logits[:, -1])\n",
    "print(logits[:, -1].argmax(dim=-1))\n",
    "\n",
    "# Sample and compare\n",
    "sample, states = dataset.model.sample_sequence(max_new_tokens=32)\n",
    "preds = model(torch.tensor([sample], dtype=torch.int64)).argmax(dim=-1).flatten().tolist()\n",
    "\n",
    "for s, pred in zip(sample[1:], preds[:-1]):\n",
    "    print(f\"Actual: {s}, Predicted: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb8e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc1/seq_len_30\n",
      "B||M = KL(Bigram || Model), M||B = KL(Model || Bigram)\n",
      "\n",
      "Model 20: B||M - 0.070, M||B - 0.063\n",
      "Model 40: B||M - 0.108, M||B - 0.087\n",
      "Model 60: B||M - 0.107, M||B - 0.068\n",
      "Model 80: B||M - 0.384, M||B - 0.149\n",
      "Model 100: B||M - 0.585, M||B - 0.168\n",
      "Model 120: B||M - 0.831, M||B - 0.174\n",
      "Model 140: B||M - 0.815, M||B - 0.172\n",
      "Model 160: B||M - 0.747, M||B - 0.171\n",
      "Model 180: B||M - 0.844, M||B - 0.172\n",
      "Model 200: B||M - 0.796, M||B - 0.173\n",
      "Model 220: B||M - 0.854, M||B - 0.172\n",
      "Model 240: B||M - 0.884, M||B - 0.172\n",
      "Model 260: B||M - 0.823, M||B - 0.171\n",
      "Model 280: B||M - 0.921, M||B - 0.172\n"
     ]
    }
   ],
   "source": [
    "model_name = 'proc1/seq_len_30'\n",
    "print(model_name)\n",
    "print('B||M = KL(Bigram || Model), M||B = KL(Model || Bigram)', end='\\n\\n')\n",
    "for i in range(20, 300, 20):\n",
    "\n",
    "    bm, mb = bigram_kl(load_model(f'{model_name}/model{i}.pt', f'{model_name}/model_cfg.pt'))\n",
    "    print(f\"Model {i}: B||M - {bm:.3f}, M||B - {mb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1d762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc1/seq_len_30\n",
      "B||M = KL(Markov || Model), M||B = KL(Model || Markov)\n",
      "\n",
      "Model 20: B||M - 0.111, M||B - 1.120\n",
      "Model 40: B||M - 0.103, M||B - 0.905\n",
      "Model 60: B||M - 0.080, M||B - 0.934\n",
      "Model 80: B||M - 0.019, M||B - 0.164\n",
      "Model 100: B||M - 0.003, M||B - 0.017\n",
      "Model 120: B||M - 0.003, M||B - 0.005\n",
      "Model 140: B||M - 0.001, M||B - 0.003\n",
      "Model 160: B||M - 0.001, M||B - 0.003\n",
      "Model 180: B||M - 0.001, M||B - 0.002\n",
      "Model 200: B||M - 0.001, M||B - 0.002\n",
      "Model 220: B||M - 0.000, M||B - 0.001\n",
      "Model 240: B||M - 0.000, M||B - 0.001\n",
      "Model 260: B||M - 0.001, M||B - 0.002\n",
      "Model 280: B||M - 0.000, M||B - 0.001\n"
     ]
    }
   ],
   "source": [
    "model_name = 'proc1/seq_len_30'\n",
    "print(model_name)\n",
    "print('B||M = KL(Markov || Model), M||B = KL(Model || Markov)', end='\\n\\n')\n",
    "for i in range(20, 300, 20):\n",
    "    try:\n",
    "        bm, mb = markov_kl(load_model(f'{model_name}/model{i}.pt', f'{model_name}/model_cfg.pt'))\n",
    "        print(f\"Model {i}: B||M - {bm:.3f}, M||B - {mb:.3f}\")\n",
    "    except Exception:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b77c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/toytrans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a9a271",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mdata[:\u001b[38;5;241m200\u001b[39m])  \u001b[38;5;66;03m# 200 train sequences\u001b[39;00m\n\u001b[1;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(dataset\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m200\u001b[39m:\u001b[38;5;241m300\u001b[39m])  \u001b[38;5;66;03m# 100 val sequences  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m complete_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([train_data, val_data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = torch.stack(dataset.data[:200])  # 200 train sequences\n",
    "val_data = torch.stack(dataset.data[200:300])  # 100 val sequences  \n",
    "complete_data = torch.cat([train_data, val_data], dim=0)\n",
    "\n",
    "datasets_dict = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data,\n",
    "    \"complete\": complete_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e30501",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_all_metrics(model, datasets_dict, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tracker' is not defined"
     ]
    }
   ],
   "source": [
    "from toy_transformer import metrics_tracker_advanced\n",
    "\n",
    "\n",
    "all_metrics = metrics_tracker_advanced.compute_all_metrics(model, datasets_dict, step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in all_metrics.items():\n",
    "    if 'gram' in k:\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30b70967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing prefix matching...\n",
      "Vocab size too small (2) for prefix matching\n",
      "Prefix matching scores for 0 attention heads:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting prefix matching...\")\n",
    "from metrics_tracker_advanced import compute_prefix_matching_score\n",
    "prefix_scores = compute_prefix_matching_score(model, num_samples=100)\n",
    "print(f\"Prefix matching scores for {len(prefix_scores)} attention heads:\")\n",
    "for k, v in list(prefix_scores.items())[:5]:  # Show first 5\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1adbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing in-context learning...\n",
      "In-context learning score: -inf\n",
      "  (More negative = better in-context learning)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting in-context learning...\")\n",
    "from metrics_tracker_advanced import compute_in_context_learning_score\n",
    "icl_score = compute_in_context_learning_score(model, num_samples=50, k1=5, k2=25)\n",
    "print(f\"In-context learning score: {icl_score:.4f}\")\n",
    "print(\"  (More negative = better in-context learning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7629cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing previous token matching...\n",
      "Cache key blocks.0.attn.pattern not found. Available keys: ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k']\n",
      "Previous token scores for 0 attention heads:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting previous token matching...\")\n",
    "from metrics_tracker_advanced import compute_previous_token_matching_score\n",
    "prev_scores = compute_previous_token_matching_score(model, num_samples=100)\n",
    "print(f\"Previous token scores for {len(prev_scores)} attention heads:\")\n",
    "for k, v in list(prev_scores.items())[:5]:  # Show first 5\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toytrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

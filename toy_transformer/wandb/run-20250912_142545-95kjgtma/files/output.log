Moving model to device:  cpu
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:06<00:14,  2.45it/s]
Epoch 1 Samples 8000 Step 124 Training Loss 0.4930770993232727
Epoch 1 Validation Loss 0.49595534801483154
Epoch 2 Samples 8000 Step 124 Training Loss 0.4825283885002136
Epoch 2 Validation Loss 0.48329704999923706
Epoch 3 Samples 8000 Step 124 Training Loss 0.4719834625720978
Epoch 3 Validation Loss 0.47519993782043457
Epoch 4 Samples 8000 Step 124 Training Loss 0.4717867076396942
Epoch 4 Validation Loss 0.4697587490081787
Epoch 5 Samples 8000 Step 124 Training Loss 0.4640764892101288
Epoch 5 Validation Loss 0.4666685163974762
Epoch 6 Samples 8000 Step 124 Training Loss 0.4698444902896881
Epoch 6 Validation Loss 0.4651142954826355
Epoch 7 Samples 8000 Step 124 Training Loss 0.4586086571216583
Epoch 7 Validation Loss 0.46414029598236084
Epoch 8 Samples 8000 Step 124 Training Loss 0.45675894618034363
Epoch 8 Validation Loss 0.4634513854980469
Epoch 9 Samples 8000 Step 124 Training Loss 0.45971277356147766
Epoch 9 Validation Loss 0.46270138025283813
Epoch 10 Samples 8000 Step 124 Training Loss 0.46067723631858826
Epoch 10 Validation Loss 0.46203258633613586
Epoch 11 Samples 8000 Step 124 Training Loss 0.45584574341773987
Epoch 11 Validation Loss 0.4629868268966675
Epoch 12 Samples 8000 Step 124 Training Loss 0.46004295349121094
Epoch 12 Validation Loss 0.4608277678489685
Epoch 13 Samples 8000 Step 124 Training Loss 0.463871031999588
Epoch 13 Validation Loss 0.46042293310165405
Epoch 14 Samples 8000 Step 124 Training Loss 0.4580078721046448
Epoch 14 Validation Loss 0.4597514271736145
Epoch 15 Samples 8000 Step 124 Training Loss 0.4533943235874176
Epoch 15 Validation Loss 0.45993176102638245
Epoch 16 Samples 8000 Step 124 Training Loss 0.463189035654068
Epoch 16 Validation Loss 0.45820555090904236
Epoch 17 Samples 8000 Step 124 Training Loss 0.4687129855155945
Epoch 17 Validation Loss 0.4577281177043915
Epoch 18 Samples 8000 Step 124 Training Loss 0.45643243193626404
Epoch 18 Validation Loss 0.4614095389842987
Epoch 19 Samples 8000 Step 124 Training Loss 0.4499620199203491
Epoch 19 Validation Loss 0.45726659893989563
Epoch 20 Samples 8000 Step 124 Training Loss 0.4513970613479614
Epoch 20 Validation Loss 0.4557045102119446
Epoch 21 Samples 8000 Step 124 Training Loss 0.4499674141407013
Epoch 21 Validation Loss 0.45792946219444275
Epoch 22 Samples 8000 Step 124 Training Loss 0.4491993188858032
Epoch 22 Validation Loss 0.4558751583099365
Epoch 23 Samples 8000 Step 124 Training Loss 0.457042932510376
Epoch 23 Validation Loss 0.455463171005249
Epoch 24 Samples 8000 Step 124 Training Loss 0.4568024277687073
Epoch 24 Validation Loss 0.453528493642807
Epoch 25 Samples 8000 Step 124 Training Loss 0.4537176191806793
Epoch 25 Validation Loss 0.45194247364997864
Epoch 26 Samples 8000 Step 124 Training Loss 0.449684202671051
Epoch 26 Validation Loss 0.4511968493461609
Epoch 27 Samples 8000 Step 124 Training Loss 0.4494398236274719
Epoch 27 Validation Loss 0.4497971832752228
Epoch 28 Samples 8000 Step 124 Training Loss 0.4546521008014679
Epoch 28 Validation Loss 0.4534456431865692
Epoch 29 Samples 8000 Step 124 Training Loss 0.4565465748310089
Epoch 29 Validation Loss 0.4491000473499298
Epoch 30 Samples 8000 Step 124 Training Loss 0.44805067777633667
Epoch 30 Validation Loss 0.4464956223964691
Epoch 31 Samples 8000 Step 124 Training Loss 0.44179442524909973
Epoch 31 Validation Loss 0.445743203163147
Epoch 32 Samples 8000 Step 124 Training Loss 0.44382768869400024
Epoch 32 Validation Loss 0.44571101665496826
Epoch 33 Samples 8000 Step 124 Training Loss 0.44582438468933105
Epoch 33 Validation Loss 0.44274473190307617
Epoch 34 Samples 8000 Step 124 Training Loss 0.43358293175697327
Epoch 34 Validation Loss 0.4419269561767578
Epoch 35 Samples 8000 Step 124 Training Loss 0.44564905762672424
Epoch 35 Validation Loss 0.44032686948776245
Epoch 36 Samples 8000 Step 124 Training Loss 0.4414571225643158
Epoch 36 Validation Loss 0.4408358335494995
Epoch 37 Samples 8000 Step 124 Training Loss 0.442792683839798
Epoch 37 Validation Loss 0.43556681275367737
Epoch 38 Samples 8000 Step 124 Training Loss 0.43796032667160034
Epoch 38 Validation Loss 0.434333860874176
Epoch 39 Samples 8000 Step 124 Training Loss 0.4291817247867584
Epoch 39 Validation Loss 0.43257591128349304
Epoch 40 Samples 8000 Step 124 Training Loss 0.43951407074928284
Epoch 40 Validation Loss 0.43044471740722656
Epoch 41 Samples 8000 Step 124 Training Loss 0.4164983928203583
Epoch 41 Validation Loss 0.4260748624801636
Epoch 42 Samples 8000 Step 124 Training Loss 0.42477330565452576
Epoch 42 Validation Loss 0.42281869053840637
Epoch 43 Samples 8000 Step 124 Training Loss 0.41281723976135254
Epoch 43 Validation Loss 0.42104485630989075
Epoch 44 Samples 8000 Step 124 Training Loss 0.4225577414035797
Epoch 44 Validation Loss 0.4240054488182068
Epoch 45 Samples 8000 Step 124 Training Loss 0.42879509925842285
Epoch 45 Validation Loss 0.4238857328891754
Epoch 46 Samples 8000 Step 124 Training Loss 0.42447471618652344
Epoch 46 Validation Loss 0.42857056856155396
Epoch 47 Samples 8000 Step 124 Training Loss 0.40818047523498535
Epoch 47 Validation Loss 0.4120808243751526
Epoch 48 Samples 8000 Step 124 Training Loss 0.405640184879303
Epoch 48 Validation Loss 0.4070811867713928
Epoch 49 Samples 8000 Step 124 Training Loss 0.4067683815956116
Epoch 49 Validation Loss 0.39814648032188416
Epoch 50 Samples 8000 Step 124 Training Loss 0.39211350679397583
Epoch 50 Validation Loss 0.3948310315608978
[34m[1mwandb[0m: [32m[41mERROR[0m The nbformat package was not found. It is required to save notebook history.

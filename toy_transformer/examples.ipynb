{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "from mealymarkov import MarkovMealyModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ozz_FILE_PATH = os.getenv('100_SAVE_PATH')\n",
    "zir_FILE_PATH = os.getenv('ZIR_SAVE_PATH')\n",
    "\n",
    "# Example small model (n=4 states, V=2 tokens) that satisfies the constraints.\n",
    "n = 3\n",
    "V = 2\n",
    "\n",
    "# We construct T^0 and T^1 so that T^0 + T^1 is row-stochastic (rows sum to 1).\n",
    "T0 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0.5]\n",
    "])\n",
    "\n",
    "T1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "model = MarkovMealyModel(n=n, V=V, T_list=[T0, T1])\n",
    "\n",
    "# By specification the default eta^0 is uniform\n",
    "print(\"Initial eta^0 =\", model.eta0)\n",
    "\n",
    "tokens, states = model.sample_sequence(max_new_tokens=3, seed=42)\n",
    "\n",
    "print(\"Generated tokens:\", tokens)\n",
    "print(\"States (eta^t) traversed:\")\n",
    "for i, s in enumerate(states):\n",
    "    print(f\"t={i} ->\", np.round(s, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930ce4c",
   "metadata": {},
   "source": [
    "Generating training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa614862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "#generating the process as discussed in the previous meet\n",
    "#for the process that generates 100*\n",
    "n = 3\n",
    "V = 2\n",
    "num_training_samples = 100\n",
    "sequences = {}\n",
    "# We construct T^0 and T^1 so that T^0 + T^1 is row-stochastic (rows sum to 1).\n",
    "T0 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0.5]\n",
    "])\n",
    "\n",
    "T1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "model = MarkovMealyModel(n=n, V=V, T_list=[T0, T1])\n",
    "for i in range(num_training_samples):\n",
    "    tokens, _ = model.sample_sequence(max_new_tokens=50)\n",
    "    sequences[i] = tokens\n",
    "with open(ozz_FILE_PATH, 'w') as fp:\n",
    "    json.dump(sequences, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ff94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generating the process as discussed in the previous meet\n",
    "#for the process that generates ZIR\n",
    "n = 3\n",
    "V = 2\n",
    "num_training_samples = 100\n",
    "sequences = {}\n",
    "# We construct T^0 and T^1 so that T^0 + T^1 is row-stochastic (rows sum to 1).\n",
    "T0 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "T1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "model = MarkovMealyModel(n=n, V=V, T_list=[T0, T1])\n",
    "for i in range(num_training_samples):\n",
    "    tokens, _ = model.sample_sequence(max_new_tokens=50)\n",
    "    sequences[i] = tokens\n",
    "with open(zir_FILE_PATH, 'w') as fp:\n",
    "    json.dump(sequences, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from toy_model import train_model, finetune_model, MarkovData\n",
    "\n",
    "T0 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "T1 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0.5, 0, 0]\n",
    "])\n",
    "\n",
    "dataset = MarkovData(n_gen=1000, gen_len=50, n_states=3, d_vocab=2, T_list=[T0, T1])\n",
    "model = train_model(\n",
    "    dataset=dataset,\n",
    "    n_layers=4,\n",
    "    d_model=4,\n",
    "    d_head=2,\n",
    "    d_mlp=16,\n",
    "    attn_only=True,\n",
    "    n_epochs=500,\n",
    "    lr=5e-3,\n",
    "    batch_size=200,\n",
    "    save_every=1000,\n",
    "    print_every=10000,\n",
    "    save_dir=None # To not to save the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf162582",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, states = dataset.model.sample_sequence(max_new_tokens=40)\n",
    "preds = model(torch.tensor(sample, dtype=torch.int64)).argmax(dim=-1).flatten().tolist()\n",
    "for s, pred in zip(sample[1:], preds[:-1]):\n",
    "    print(f'Actual: {s}, Predicted: {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = finetune_model(model, dataset, n_epochs=5, save_dir=None) # Add additional arguments as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(torch.tensor([[0,1,1,0,1,0,0,1,1,0],\n",
    "                                 [1,0,1,1,0,1,0,0,1,1],\n",
    "                                 [1,0,0,1,0,0,1,0,0,1]], dtype=torch.int64))\n",
    "print(logits[:, -1, :])\n",
    "print(logits[:, -1, :].argmax(dim=-1))\n",
    "# Ground truth values: [1, 0, R]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ecf7e",
   "metadata": {},
   "source": [
    "Verifying whether a given probability distribution is what the Markov model would have provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the verify_sequence function\n",
    "print('\\n' + '='*50)\n",
    "print('Testing verify_sequence function:')\n",
    "print('='*50)\n",
    "\n",
    "# Create a test sequence and probability distribution\n",
    "test_sequence = ['0', '1', '0']  # String representations of token indices\n",
    "test_probs = [\n",
    "    [0.833, 0.166],\n",
    "    [0.7, 0.3],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "print(f\"Test sequence: {test_sequence}\")\n",
    "print(f\"Test probabilities: {test_probs}\")\n",
    "\n",
    "# Verify the sequence\n",
    "is_converged, conv_pos = model.verify_sequence(test_sequence, test_probs, tolerance=0.1)\n",
    "\n",
    "print(f\"\\nVerification result:\")\n",
    "print(f\"  Is converged: {is_converged}\")\n",
    "print(f\"  Convergence position: {conv_pos}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
